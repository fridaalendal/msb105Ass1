---
title: "Is reproducibility good enough?"
author: "Frida Alendal"
date: last-modified
date-format: "dddd D MMM, YYYY"
csl: apa7.csl
lang: en-GB
format:
  html: default
  typst:
    papersize: a4
  pdf:
    documentclass: article
    number-sections: true
    keep-tex: true
    papersize: A4
    fig-pos: "H"
bibliography: reproducibility-2.bib
abstract: "This paper explores the importance of reproducibility and replicability in scientific research. It reviews key challenges such as publication bias, type I errors, and the replication crisis, while also considering solutions including open science practices and computable documents like Quarto. The discussion argues that replicability is an aspirational goal, but reproducibility must serve as the minimum standard to maintain credibility and trust in science." 
---



## Introduction

What is this paper about?

Reproducibility and replicability lie at the core of modern scientific practice. Scientific knowledge is built on the assumption that results can be verified, either by reusing the same data and methods (reproducibility) or by conducting new studies with comparable designs (replicability). However, numerous studies and large-scale projects have demonstrated that a substantial share of published findings cannot be reproduced or replicated, as argued by @ioannidis2005. This phenomenon has contributed to what is often referred to as the "replication crisis", initially recognized in psychology but increasingly evident across various scientific disciplines.

This assignment discusses what reproducibility entails, why it is a fundamental requirement for scientific progress, and the consequences when this ideal is not fulfilled. Furthermore, it highlights key challenges such as publication bias, type I errors, and incentive structures in academia, before turning to possible solutions. Among these, the use of open data and code archives, as well as the development of "computable documents" through tools such as R Markdown and RStudio, will be emphasized. The purpose is to demonstrate how technological and methodological innovations can support a more robust, transparent, and trustworthy scientific practice.


## Literature review

### Reproduccibility and the credibility of scientific evidence
Reproducibility—the ability to recompute results given the same data and code—has become a central benchmark for credible science, particularly when full, independent replication is infeasible [@peng2011; @goodman2016b]. Evidence from large-scale efforts shows substantial gaps: in psychology, only about a third to half of classic effects replicate under high-powered, pre-registered protocols [@opensciencecollaboration2015; @klein2018b]. Long-standing methodological critiques suggest that small studies, flexible analysis choices, publication bias, and selective reporting inflate false-positive rates and exaggerate effect sizes [@ioannidis2005b; @simmons2011; @young2008; @iyengar1988b; @rosenthal1979]. Editorial calls have therefore urged stronger standards and incentives for openness and verification [@mcnutt2014; @jasny2011b]. Recent work further emphasizes that open data alone is insufficient without detailed protocols and analytic transparency [@hardwicke2020].

### Open research culture and olicy responses
Cultural and policy mechanisms can shift norms. The Transparency and Openness Promotion (TOP) guidelines outline modular practices (e.g., data/code sharing, preregistration, replication) that journals and funders can adopt to raise the floor for credibility [@nosek2015]. At the policy level, European and funder initiatives have promoted access to publications and data, accelerating alignment around FAIR ideas and durable data citation [@2015; @starr2015; @celebi2020]. Open access growth further increases visibility and reuse, with measurable citation advantages when access barriers are reduced [@piwowar2018]. Funders now emphasize that reproducibility is part of research integrity, and failure to provide data or code may be treated as misconduct [@nationalacademies2019].

### Computational reproducibility: workflows, tooling, and literate programming
Because so much modern inference is computational, transparent workflows are essential [@peng2011]. Literate programming proposes weaving prose and code so that analyses are executable and inspectable [@knuth1984b; @knuth1992b]. In the R ecosystem, R Markdown and knitr operationalize this idea, enabling dynamic documents that integrate narrative, code, and outputs [@xie2014; @xie2015; @xie2018; @allaire2020b]. Such tools reduce “analytic drift,” make results re-runnable across environments, and align with good scientific writing practices [@gentleman2003; @gentleman2007b; @gentleman2005b]. More broadly, well-structured data workflows and containerized environments (e.g., Docker/Gradle) lower barriers for others to reproduce simulations and analyses [@broman2018; @elmenreich2019]. Version control (Git) underpins these practices by documenting changes, branching experimentation, and facilitating transparent collaboration [@ram2013; @conyersb]. Container-based reproducibility has recently expanded to cloud-native platforms that guarantee portable, archived research environments [@stodden2018].

### Data sharing, identifiers, and metdata
Reproducibility depends on access to the exact inputs and sufficient contextual metadata. Persistent identifiers for datasets (e.g., DOIs via DataCite) and machine-actionable metadata enable stable citation, discovery, and automated reuse [@brase2009b; @starr2015; @bechhofer2013b]. Clear, consistent data organization reduces clerical errors and speeds secondary analysis [@broman2018; @ellis2017]. Where domain repositories and journal archives exist, their policies critically affect practical replicability; uneven archival practices have historically limited reanalysis [@mccullough2008a; @zotero-321]. However, metadata quality is often inconsistent, and ongoing initiatives highlight the need for standardized schema and minimal reporting guidelines [@wilkinson2016].

### Discipline-specific lessons and exemplar domains
Domains with high-throughput measurement (e.g., genomics) highlighted early the need for sharable pipelines and transparent classification procedures [@golub1999c]. These lessons generalize: reproducible compendia and executable papers help bridge the gap between static PDFs and live analyses, making methodological claims auditable and extensible [@nust2017; @gentleman2003; @markowetz2015a]. Community surveys consistently emphasize that reproducibility should not require prohibitive effort—hence the value of opinionated tooling and conventions that work “out of the box” [@elmenreich2019; @markowetz2015a]. New surveys also reveal differences between fields: computational biology has embraced reproducibility faster than economics or psychology, due to stronger culture and infrastructure [@stodden2016].

### Putting it into practice in R
At the project level, using scripted data ingestion, tidy transformations, and declarative visualization encourages clarity and reduces off-pipeline edits [@wickham2016; @grolemundb]. Dynamic documents (R Markdown) compiled via knitr promote single-source truth for text, code, tables, and figures [@xie2018; @xie2015]. Reproducibility is further strengthened by pinning package versions, documenting session info, and using containers or renv-style dependency locks alongside the base R platform [@rcoreteam2020a; @xie2020]. Finally, journals’ early arguments to “publish your code—even if it’s not perfect” remain pragmatic and impactful: sharing working code is better than sharing none [@barnes2010b]. Increasingly, funders and journals require not only code, but also complete computational notebooks, versioned environments, and container manifests as part of supplementary materials [@binder2019].

### Summary
The literature converges on a practical recipe: align incentives for openness [@nosek2015; @mcnutt2014], ensure durable access and citation of data [@starr2015; @brase2009b], adopt executable, literate workflows [@knuth1984b; @xie2014; @xie2018], structure data and metadata well [@broman2018; @ellis2017; @wilkinson2016], and normalize version control and containerization [@ram2013; @elmenreich2019; @stodden2018]. These measures collectively address known threats—selective reporting, analytic flexibility, and publication bias [@ioannidis2005b; @simmons2011; @iyengar1988b]—and help close the gap between nominal and actual reproducibility [@opensciencecollaboration2015; @klein2018b; @stodden2016].



Smart stuff from others about the topic.

Use a least 20 citations, a least 5 of them must be new (not from the provided .bib file).

Use both in-line and normal citations.

Example:

@gentleman2005 argues that bla bla bla.
On the other hand it's claimed that bla bla [@barbalorenaa.2018; @bartlett2008].

## Discussion of the reseach question

-   Should replicability be the norm or is this to much to ask for now?
-   Can Quarto documents help with reproducibility?
-   What problems remains and how can these be solved?

## Conclusion



## References

and

-   Version number and reference to packages used
-   R version used
