---
title: "Is reproducibility good enough?"
author: "Frida Alendal"
date: last-modified
date-format: "dddd D MMM, YYYY"
csl: apa7.csl
lang: en-GB
format:
  html: default
  typst:
    papersize: a4
  pdf:
    documentclass: article
    number-sections: true
    keep-tex: true
    papersize: A4
    fig-pos: "H"
bibliography: reproducibility-2.bib
abstract: "A very short abstract. Put the abstract text here. One or two paragraphs summarising what follows below."
---

## Introduction

What is this paper about?

Reproducibility and replicability lie at the core of modern scientific practice. Scientific knowledge is built on the assumption that results can be verified, either by reusing the same data and methods (reproducibility) or by conducting new studies with comparable designs (replicability). However, numerous studies ans large-scale projects have demonstarted that a substantial share of published findings cannot be reproduced or replicated (McNutt, 2014; loannidis, 2005). This phenomenon has contributed to what is often referred to as the "replication crisis", initially recognized in psychology but increasingly evident across various scientific disciplines.

This assignment discusses what reproducibility entails, why it os a fundamental requirement for scientific progress, and the consequences when this ideal is not fulfilled. Furthermore, it highlights key challenges such as publication bias, type 1 errors, and incentive structures in academia, before turning to possible solutions. Among these, the use of open data and code archives, as well as the development of "computable documents" through tools such as R Markdown and RStudio, will be emphasized. The purpose is to demonstrate how technological and methodological innovations can support a more robust, transparent, and trustworthy scientific practice.


## Literature review

Reproducibility is often described as a cornerstone of scientific progress, yet there is no single agreed-upon definition of the concept. According to @goodman2016, reproducibility refers to the ability to obtain the same results when re-analysing the original data using the same methods. Replicability, on the other hand, requires that new data collected under the same conditions yield consistent findings (see also @nosek2015). This distinction in important, since much of the ongoing debate in the so-called "replication crisis" relates not only to computational reproducibility but also to the broader credibility of research results. 

One major challenge to reproducibility is **Publication bias**. Studies with statistically significant or "positive" results are more likley to be published, whereas null finding often remain in the "filer drawer" [@rosenthal1979]. As a consequence, the published literature can become skewed towards type 1 errors, that is, false positives where the null hypothesis id incorrectly rejected [@simmons2011]. For instance, @ioannidis2005 provocatively argued that "most published research findings are false", a claim supported by multiple large replication projects. 

The replication crisis became particularly visible within psychology, where largescale projects attempted to replicate landmark studies. The Open Science Collaboration led by @nosek2015 found that replication effects were, on average, only half the size of original effects, and that less than 40% of replication attempts produced statistically significant results. Other multi-lab projects such as @klein2018 confirmed substantial variation in replicability across settings, suggesting that context and methodology play a crucial role in determining outcomes. Similar problems have been identified in other fields, including economies, where attempts at replicating influential studies often failed due to missing data, poor documentation, or lack of available code [@dewald1986]. The JMCB project (1982) revealed that only a handful of studies could be fully replicated out of more than seventy published, highlighting systemic problems with data sharing and transparency in economics [@mccullough2008].

Several solutions have been proposed to address these issues. Journals have established data and code archives, though their effectiveness remains debated [@mccullough2008]. Initiatives such as DataCite provide digital object identifiers (DOIs) for datasets to make them more accessible and citable [@brase2009]. Furthermore, the development of **computable documents** has become a central innovation. As argued by @gentleman2007 and @schwab1995, combining text, data, and code in one integrated document enables readers to reproduce the analysis directly. In the R ecosystem, this is implemented through tools such as Sweave [@leisch2002], knitr [@xie2014; @xie2015; @xie2020], and R Markdown [@allaire2020], which are now widely used in data science and statistics. These tools embody the vision of "dynamic documents" where the code, methods, and results are bundled together, thereby making reproducibility a natural byproduct of the research process [@peng2011].

In summary, the literature suggests that while reproducibility is considered essential, achieving replicability across studies remains much more challenging. The structural issues of publication bias, statistical errors, and inadequate data sharing continue to threaten the credibility of science. At the same time, technological advances such as open repositories, data citation standards, and computable documents offer promising avenues for addressing the reproducibility crisis. Whether these innovations will be sufficient to shift scientific culture towards full transparency and replicability, however, remains an open question. 

Smart stuff from others about the topic.

Use a least 20 citations, a least 5 of them must be new (not from the provided .bib file).

Use both in-line and normal citations.

Example:

@gentleman2005 argues that bla bla bla.
On the other hand it's claimed that bla bla [@barbalorenaa.2018; @bartlett2008].

## Discussion of the reseach question

-   Should replicability be the norm or is this to much to ask for now?
-   Can Quarto documents help with reproducibility?
-   What problems remains and how can these be solved?

## Conclusion

## References

and

-   Version number and reference to packages used
-   R version used
