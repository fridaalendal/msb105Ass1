---
title: "Is reproducibility good enough?"
author: "Frida Alendal"
date: last-modified
date-format: "dddd D MMM, YYYY"
csl: apa7.csl
lang: en-GB
format:
  html: default
  typst:
    papersize: a4
  pdf:
    documentclass: article
    number-sections: true
    keep-tex: true
    papersize: A4
    fig-pos: "H"
bibliography: reproducibility-2.bib
abstract: "This paper explores the importance of reproducibility and replicability in scientific research. It reviews key challenges such as publication bias, type I errors, and the replication crisis, while also considering solutions including open science practices and computable documents like Quarto. The discussion argues that replicability is an aspirational goal, but reproducibility must serve as the minimum standard to maintain credibility and trust in science." 
---


## Introduction
Reproducibility and replicability lie at the core of modern scientific practice. Scientific knowledge is built on the assumption that results can be verified, either by reusing the same data and methods (reproducibility) or by conducting new studies with comparable designs (replicability). However, numerous studies and large-scale projects have demonstrated that a substantial share of published findings cannot be reproduced or replicated, as argued by @ioannidis2005. This phenomenon has contributed to what is often referred to as the "replication crisis", initially recognized in psychology but increasingly evident across various scientific disciplines.

This assignment discusses what reproducibility entails, why it is a fundamental requirement for scientific progress, and the consequences when this ideal is not fulfilled. Furthermore, it highlights key challenges such as publication bias, type I errors, and incentive structures in academia, before turning to possible solutions. Among these, the use of open data and code archives, as well as the development of "computable documents" through tools such as R Markdown and RStudio, will be emphasized. The purpose is to demonstrate how technological and methodological innovations can support a more robust, transparent, and trustworthy scientific practice. This assignment therefore asks whether reproducibility alone is good enough, or whether replicability should be demanded as the standard.


## Literature review

### Reproducibility and the credibility of scientific evidence
@peng2011 emphasizes that reproducibility, the ability to recompute results given the same data and code, is a central benchmark for credible science, particularly when full, independent replication is infeasible. This view is also supported by @goodman2016b, who stresses that reproducibility serves as the minimum requirement for credible claims. Evidence from psychology shows substantial gaps, with large-scale projects revealing that only a fraction of classic effects replicate under high-powered, pre-registered protocols [@opensciencecollaboration2015; @klein2018b]. Critics such as @ioannidis2005b have long argued that small studies and methodological flexibility and selective reporting inflate false-positive rates, highlighting the broader replication crisis that undermines trust in research. 

### Open research culture and policy responses
@nosek2015 introduce the Transparency and Openness Promotion (TOP) guidelines, which outline practices such as data sharing, preregistration, and replication. At the policy level, European initiatives have promoted access to publications and data, accelerating alignment around FAIR principles and durable citation [@starr2015]. Open access growth further increases visibility and reuse, with measurable citation advantages [@piwowar2018]. Funders also emphasize that reproducibility is part of research integrity, and failure to provide data or code may be treated as misconduct [@nas2019].

### Computational reproducibility: workflows, tooling, and literate programming
Because so much modern inference is computational, transparent workflows are essential [@peng2011]. Literate programming, first described by @knuth1984b, weaves prose and code so that analyses are executable and inspectable. In the R ecosystem, dynamic documents such as R Markdown integrate narrative, code, and outputs [@xie2015; @allaire2020b]. These tools reduce analytic drift and align with good scientific writing practices [@gentleman2005b]. More recently, Quarto has expanded these possibilities by combining code, data, text, and results in a single source document, making analyses easier to reproduce and share across disciplines [@quarto2023].

### Data sharing, identifiers, and metadata
Reproducibility depends on access to the exact inputs and sufficient contextual metadata. Persistent identifiers such as DOIs, along with machine-actionable metadata, enable stable citation and automated reuse [@brase2009b]. Clear, consistent data organization further reduces clerical errors and supports secondary analysis [@broman2018]. To improve consistency, the FAIR guidelines highlight the importance of standardized schema and minimal reporting [@wilkinson2016]. 

### Discipline-specific lessons and exemplar domains
@golub1999c note that high-throughput fields such as genomics emphasized early the need for sharable pipelines and transparent procedures. These lessons generalize to other disciplines, where reproducible compendia and executable papers help bridge the gap between static publications and live analyses [@markowetz2015a]. Community surveys underline that reproducibility should not require prohibitive effort, and reproducibility cultures differ between disciplines. @stodden2016 claim that computational biology is moving faster than psychology or economics in this regard.

### Putting it into practice in R
At the project level, using scripted data ingestion, tidy transformations, and declarative visualization encourages clarity and reduces off-pipeline edits [@wickham2016]. Dynamic documents (R Markdown) compiled via knitr allow text, code, and results to be integrated in a single source, which strengthens reproducibility [@xie2015]. This is further supported by documenting session info, pinning package versions, and using reproducible environments such as renv or containers [@rcoreteam2020a]. Journals have also highlighted the practical value of publishing working code, even if imperfect, as it enables others to verify and build upon results [@barnes2010b]. More recently, reproducibility has been extended through platforms that allow complete notebooks and environments to be shared online [@binder2019].

### Summary
The literature points to a set of common principles that support more credible and transparent research. These include stronger incentives for openness, reliable access to data, the use of executable and literate workflows, clear standards for data and metadata, and the adoption of reproducible tools such as version control and containerization. Together, these measures help address well-known threats such as selective reporting and analytic flexibility, and they contribute to closing the gap between nominal and actual reproducibility. 


## Discussion of the research question
As outlined in the literature review, reproducibility and replicability are closely connected but not identical. A central question is whether replicability should be the norm. From a philosophical perspective, replication is essential to establish generalisable knowledge. Without it, findings risk remaining anecdotal. However, demanding universal replicability may be too ambitious in the short term. Contextual differences, limited resources, and methodological diversity make replication challenging in fields such as psychology and biomedicine. It may therefore be more practical to promote transparency and reproducibility as immediate priorities, while striving for replicability as a long-term goal. 

Technological tools such as Quarto documents offer significant progress for reproducibility. By integrating code, data, text, and results in a single file, Quarto ensures that analyses can be rerun and verified by others. This reduces the risks of  selective reporting, or errors that weaken credibility. Moreover, Quarto´s ability to output to multiple formats makes it easier to share reproducible research across disciplines. However, while Quarto improves computational reproducibility, it cannot guarantee replicability, which requires new data and independent confirmation. 

Several challenges remain unresolved. Publication bias continues to distort the scientific record, while incentive structures in academia reward novelty over verification. Replication studies are rarely valued or published, reducing researchers´ motivation to attempt them. Technical barriers such as missing metadata, proprietary software, and inadequate infrastructure further hinder accessibility. Addressing these issues requires both cultural change and technical solutions. Policies mandating data and code sharing, preregistration of studies, and recognition of replication work as valuable contributions are crucial steps forward.

Ultimately, a combination of open science practices and technological innovation offers the best path forward. While replicability may not yet be universal, strengthening reproducibility through transparency and tools like Quarto can help rebuild trust and credibility in research.


## Conclusion

Reproducibility and replicability are fundamental to scientific progress, yet the replication crisis has revealed major shortcomings across disciplines. Problems such as publication bias, type I errors, and inadequate data sharing undermine trust in research, while cultural and institutional incentives often work against replication. At the same time, new practices and technologies are helping to address these challenges. Open data initiatives, data citation standards, and dynamic documents in R and Quarto make it easier to reproduce research, thereby increasing transparency and accountability. 

This paper concludes that while replicability should remain an aspirational goal, reproducibility must be treated as the minimum standard. By combining cultural reforms with technological tools, the scientific community can take meaningful steps to restore credibility and ensure that research provides a reliable foundation for future knowledge. Future improvements in training, incentives, and infrastructure will be crucial to ensure that reproducibility becomes not just a requirement, but a natural part of scientific culture. 


### Appendix: R Session Info

```{r}
sessionInfo()

```

## References
